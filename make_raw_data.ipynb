{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCInside 게시글 크롤링을 시작합니다.\n",
      "'https://gall.dcinside.com/board/lists?id=baseball_new11'에서 최대 300개의 게시글 링크를 수집합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "게시글 링크 수집 중:   0%|          | 0/300 [07:04<?, ?it/s]\n",
      "게시글 링크 수집 중: 100%|██████████| 300/300 [00:06<00:00, 46.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "총 300개의 게시글 링크 수집 완료.\n",
      "\n",
      "게시글 내용 스크레이핑 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "게시글 내용 수집 중: 100%|██████████| 300/300 [03:12<00:00,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "수집된 데이터를 './data/crawled_posts_basic.csv' 파일로 저장합니다.\n",
      "'./data/crawled_posts_basic.csv' 파일 저장 완료. 총 300개 게시글 정보 저장.\n",
      "\n",
      "프로그램이 종료되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import csv\n",
    "import re # get_post_content 함수에서 사용\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import random # time.sleep()에 사용\n",
    "\n",
    "# --- 1. 설정 ---\n",
    "# DC_GALLERY_URL = \"https://gall.dcinside.com/mgallery/board/lists/?id=platinalab\"\n",
    "DC_GALLERY_URL = \"https://gall.dcinside.com/board/lists?id=baseball_new11\"\n",
    "BASE_DC_URL = \"https://gall.dcinside.com\" # 링크 생성 시 사용\n",
    "TARGET_POST_COUNT = 300 # 수집할 목표 게시글 수\n",
    "OUTPUT_CSV_FILE = \"./data/crawled_posts_basic.csv\" # 저장될 CSV 파일명 변경\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# --- 2. 웹 스크레이핑 함수 ---\n",
    "\n",
    "def get_post_links_and_titles(gallery_url, target_count):\n",
    "    \"\"\"갤러리 목록 페이지에서 게시글 링크와 제목을 수집합니다.\"\"\"\n",
    "    post_details = []\n",
    "    page = 1\n",
    "    collected_count = 0\n",
    "    # tqdm의 total을 target_count로 설정하여 진행률 표시\n",
    "    pbar = tqdm(total=target_count, desc=\"게시글 링크 수집 중\")\n",
    "\n",
    "    while collected_count < target_count:\n",
    "        try:\n",
    "            current_url = f\"{gallery_url}&page={page}\"\n",
    "            response = requests.get(current_url, headers=HEADERS, timeout=10)\n",
    "            response.raise_for_status() # HTTP 오류 발생 시 예외 발생\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # 일반 게시글 선택자 (공지 제외)\n",
    "            posts_in_page = soup.select('table.gall_list tbody tr.ub-content.us-post')\n",
    "\n",
    "            if not posts_in_page:\n",
    "                if page == 1 and collected_count == 0:\n",
    "                    print(f\"페이지 {page}에서 게시글을 찾을 수 없습니다. CSS 선택자('table.gall_list tbody tr.ub-content.us-post')가 정확한지, \"\n",
    "                          \"또는 웹사이트 구조가 변경되었는지 확인해주세요.\")\n",
    "                else:\n",
    "                    print(f\"페이지 {page}에서 더 이상 게시글을 찾을 수 없어 수집을 중단합니다.\")\n",
    "                break # 더 이상 게시글이 없으면 루프 종료\n",
    "\n",
    "            for post_row in posts_in_page:\n",
    "                # 제목 링크 선택 (공지 아이콘 클래스 제외)\n",
    "                title_tag = post_row.select_one('td.gall_tit a:not([class*=\"icon_notice\"])')\n",
    "\n",
    "                if title_tag and title_tag.has_attr('href'):\n",
    "                    title = title_tag.get_text(strip=True)\n",
    "                    relative_link = title_tag['href']\n",
    "\n",
    "                    if relative_link.startswith('/'):\n",
    "                        link = BASE_DC_URL + relative_link\n",
    "                    else:\n",
    "                        link = relative_link\n",
    "                    \n",
    "                    # 중복 링크 방지 (이미 수집된 링크인지 확인)\n",
    "                    if not any(d['link'] == link for d in post_details):\n",
    "                        post_details.append({'title': title, 'link': link})\n",
    "                        collected_count += 1\n",
    "                        pbar.update(1) # 진행 바 업데이트\n",
    "\n",
    "                    if collected_count >= target_count:\n",
    "                        break # 목표 개수 도달 시 내부 루프 종료\n",
    "            \n",
    "            if collected_count >= target_count:\n",
    "                break # 목표 개수 도달 시 외부 루프 종료\n",
    "\n",
    "            page += 1\n",
    "            time.sleep(random.uniform(0.5, 1.5)) # 서버 부하 감소를 위한 딜레이\n",
    "\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"오류: 페이지 {page} 요청 시간 초과. 다음 페이지로 넘어갑니다.\")\n",
    "            page += 1\n",
    "            time.sleep(1)\n",
    "            continue\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"오류: 페이지 {page} 스크레이핑 중 요청 오류 발생 - {e}\")\n",
    "            break \n",
    "        except Exception as e:\n",
    "            print(f\"알 수 없는 오류 발생 (페이지 {page} 처리 중): {e}\")\n",
    "            break\n",
    "            \n",
    "    pbar.close() # 진행 바 닫기\n",
    "    if not post_details and target_count > 0:\n",
    "        print(\"수집된 게시글이 전혀 없습니다. 프로그램 초기의 CSS 선택자, 네트워크 연결, 또는 대상 웹사이트의 접근성을 확인해주세요.\")\n",
    "    return post_details[:target_count]\n",
    "\n",
    "def get_post_content(post_url):\n",
    "    \"\"\"개별 게시글 페이지에서 본문 내용을 가져옵니다.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(post_url, headers=HEADERS, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        content_div = soup.select_one('div.writing_view_box')\n",
    "        \n",
    "        if content_div:\n",
    "            # 불필요한 태그 제거\n",
    "            for unwanted_tag in content_div.find_all(['script', 'style', 'iframe', 'ins', 'figure', \n",
    "                                                     '.adv_bottom_title', 'div.btn_recommend_box', \n",
    "                                                     'div.json_dccon_viewer', 'div.social_area',\n",
    "                                                     'div.gallery_info_bottom', 'div.related_cont']):\n",
    "                unwanted_tag.decompose()\n",
    "            \n",
    "            text_parts = []\n",
    "            for element in content_div.descendants:\n",
    "                if isinstance(element, str): # NavigableString\n",
    "                    stripped_text = element.strip()\n",
    "                    if stripped_text:\n",
    "                        text_parts.append(stripped_text)\n",
    "                elif element.name == 'br':\n",
    "                    if text_parts and text_parts[-1] != '\\n' and text_parts[-1].strip() != '':\n",
    "                         text_parts.append('\\n')\n",
    "\n",
    "            content = \" \".join(text_parts)\n",
    "            content = re.sub(r'\\s+', ' ', content).strip()\n",
    "            content = content.replace(' \\n ', '\\n').replace('\\n ', '\\n').replace(' \\n', '\\n')\n",
    "\n",
    "            # content_excerpt를 위해 전체 내용을 반환 (길이 제한은 메인 로직에서)\n",
    "            return content\n",
    "        else:\n",
    "            return \"본문 내용을 찾을 수 없습니다.\"\n",
    "            \n",
    "    except requests.exceptions.Timeout:\n",
    "        return f\"게시글 내용 로드 실패 (타임아웃): {post_url}\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"게시글 내용 로드 실패: {e}\"\n",
    "    except Exception as e:\n",
    "        return f\"알 수 없는 오류로 내용 로드 실패: {e}\"\n",
    "\n",
    "# --- 3. 메인 로직 ---\n",
    "print(f\"DCInside 게시글 크롤링을 시작합니다.\")\n",
    "\n",
    "# 데이터 저장 디렉토리 생성\n",
    "if not os.path.exists('./data'):\n",
    "    os.makedirs('./data')\n",
    "    print(\" './data' 디렉토리가 생성되었습니다.\")\n",
    "\n",
    "print(f\"'{DC_GALLERY_URL}'에서 최대 {TARGET_POST_COUNT}개의 게시글 링크를 수집합니다.\")\n",
    "posts_to_scrape = get_post_links_and_titles(DC_GALLERY_URL, TARGET_POST_COUNT)\n",
    "\n",
    "if not posts_to_scrape:\n",
    "    print(\"수집할 게시글이 없습니다. 프로그램을 종료합니다.\")\n",
    "    exit() # return 대신 exit() 사용 (함수 외부이므로)\n",
    "\n",
    "print(f\"\\n총 {len(posts_to_scrape)}개의 게시글 링크 수집 완료.\")\n",
    "all_scraped_data = []\n",
    "\n",
    "print(\"\\n게시글 내용 스크레이핑 중...\")\n",
    "for post_detail in tqdm(posts_to_scrape, desc=\"게시글 내용 수집 중\"):\n",
    "    post_title = post_detail['title']\n",
    "    post_link = post_detail['link']\n",
    "    \n",
    "    post_content_full = get_post_content(post_link)\n",
    "\n",
    "    content_excerpt = \"내용 없음\"\n",
    "    if post_content_full and not post_content_full.startswith(\"본문 내용을 찾을 수 없습니다.\") and not post_content_full.startswith(\"게시글 내용 로드 실패\"):\n",
    "        # 본문 미리보기는 200자로 제한하고, 줄바꿈은 공백으로 대체\n",
    "        content_excerpt = post_content_full[:200].replace('\\n', ' ') + \"...\"\n",
    "    elif post_content_full.startswith(\"본문 내용을 찾을 수 없습니다.\"):\n",
    "        content_excerpt = \"본문 내용을 찾을 수 없습니다.\"\n",
    "    elif post_content_full.startswith(\"게시글 내용 로드 실패\"):\n",
    "        content_excerpt = post_content_full # 오류 메시지 그대로 사용\n",
    "\n",
    "    all_scraped_data.append({\n",
    "        \"title\": post_title,\n",
    "        \"link\": post_link,\n",
    "        \"content_excerpt\": content_excerpt\n",
    "    })\n",
    "    \n",
    "    # 각 게시물 처리 후 짧은 휴식 (서버 부하 감소)\n",
    "    time.sleep(random.uniform(0.3, 0.8)) # 딜레이 약간 줄임\n",
    "\n",
    "# --- 4. 결과를 CSV 파일로 저장 ---\n",
    "print(f\"\\n수집된 데이터를 '{OUTPUT_CSV_FILE}' 파일로 저장합니다.\")\n",
    "if not all_scraped_data:\n",
    "    print(\"저장할 데이터가 없습니다.\")\n",
    "else:\n",
    "    try:\n",
    "        # CSV 파일 저장 디렉토리 확인 및 생성\n",
    "        output_dir = os.path.dirname(OUTPUT_CSV_FILE)\n",
    "        if output_dir and not os.path.exists(output_dir): # output_dir이 빈 문자열이 아니고, 존재하지 않으면\n",
    "            os.makedirs(output_dir)\n",
    "            print(f\"저장 디렉토리 '{output_dir}'가 생성되었습니다.\")\n",
    "\n",
    "        with open(OUTPUT_CSV_FILE, mode='w', newline='', encoding='utf-8-sig') as file:\n",
    "            # 저장할 필드명 정의\n",
    "            writer = csv.DictWriter(file, fieldnames=[\"title\", \"link\", \"content_excerpt\"])\n",
    "            writer.writeheader() # 헤더 작성\n",
    "            writer.writerows(all_scraped_data) # 모든 데이터 한번에 작성\n",
    "            \n",
    "        print(f\"'{OUTPUT_CSV_FILE}' 파일 저장 완료. 총 {len(all_scraped_data)}개 게시글 정보 저장.\")\n",
    "    except IOError as e:\n",
    "        print(f\"오류: '{OUTPUT_CSV_FILE}' 파일 저장에 실패했습니다. - {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"CSV 저장 중 알 수 없는 오류 발생: {e}\")\n",
    "\n",
    "print(\"\\n프로그램이 종료되었습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "filter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
